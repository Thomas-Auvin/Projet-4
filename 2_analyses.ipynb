{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7046579b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from fonctions.fonctions_features import FeatureEngineer\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector as selector\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, cross_validate\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score, \n",
    "    classification_report, confusion_matrix,\n",
    "    confusion_matrix, roc_auc_score, \n",
    "    average_precision_score, precision_recall_curve,\n",
    "    precision_recall_fscore_support,\n",
    ")\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "import importlib\n",
    "import fonctions.fonctions_features as ff\n",
    "importlib.reload(ff)\n",
    "from fonctions.fonctions_features import FeatureEngineer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f052b234",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/jeu_donnee_RH_complet_transforme.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c106d896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy           : 0.8401360544217688\n",
      "Balanced accuracy  : 0.5\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91       247\n",
      "           1       0.00      0.00      0.00        47\n",
      "\n",
      "    accuracy                           0.84       294\n",
      "   macro avg       0.42      0.50      0.46       294\n",
      "weighted avg       0.71      0.84      0.77       294\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[247   0]\n",
      " [ 47   0]]\n",
      "\n",
      "Baseline (majorité dans tout le set) : 0.8387755102040816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thoma\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\projet4-KgNUqIF9-py3.13\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\thoma\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\projet4-KgNUqIF9-py3.13\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\thoma\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\projet4-KgNUqIF9-py3.13\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "#DUMMY\n",
    "\n",
    "# --- 2) Préparation des données d’entrée \n",
    "data_pd = data.copy()\n",
    "\n",
    "X = data_pd.drop(columns=[\"a_quitte_l_entreprise\"])\n",
    "y = data_pd[\"a_quitte_l_entreprise\"].map({\"Non\": 0, \"Oui\": 1}).astype(int)\n",
    "\n",
    "# --- 3) Pipeline complète avec TON FE + préprocessing classique + XGB ---\n",
    "numeric_sel = selector(dtype_include=[\"number\", \"bool\"])   # inclut les bools comme numériques\n",
    "categorical_sel = selector(dtype_exclude=[\"number\", \"bool\"])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler())   # centrage + mise à l'échelle des numériques\n",
    "        ]), numeric_sel),\n",
    "        (\"cat\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        ]), categorical_sel),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "pipe_dummy = Pipeline(steps=[\n",
    "    (\"fe\", FeatureEngineer()),   # <- ton feature engineering, sans fuite\n",
    "    (\"prep\", preprocess),\n",
    "    (\"clf\", DummyClassifier(strategy=\"most_frequent\", random_state=42)),\n",
    "])\n",
    "\n",
    "# --- 4A) En TRAIN / TEST \"classique\" (pas de fuite) ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "pipe_dummy.fit(X_train, y_train) \n",
    "\n",
    "y_pred_dummy = pipe_dummy.predict(X_test)\n",
    "\n",
    "print(\"Accuracy           :\", accuracy_score(y_test, y_pred_dummy))\n",
    "print(\"Balanced accuracy  :\", balanced_accuracy_score(y_test, y_pred_dummy))\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred_dummy))\n",
    "print(\"\\nConfusion matrix:\\n\", confusion_matrix(y_test, y_pred_dummy))\n",
    "\n",
    "# (Optionnel) contrôle rapide du baseline théorique\n",
    "print(\"\\nBaseline (majorité dans tout le set) :\", y.value_counts(normalize=True).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2886e4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LogisticRegression (balanced) ===\n",
      "Accuracy           : 0.6462585034013606\n",
      "Balanced accuracy  : 0.7205616332156086\n",
      "Avg Precision (PR) : 0.589014756777982\n",
      "ROC AUC            : 0.8204841071582393\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.61      0.74       247\n",
      "           1       0.29      0.83      0.43        47\n",
      "\n",
      "    accuracy                           0.65       294\n",
      "   macro avg       0.62      0.72      0.59       294\n",
      "weighted avg       0.84      0.65      0.69       294\n",
      "\n",
      "Confusion:\n",
      " [[151  96]\n",
      " [  8  39]]\n",
      "[precision] train=0.411±0.007 | test=0.362±0.014\n",
      "[recall] train=0.794±0.010 | test=0.730±0.032\n",
      "[roc_auc] train=0.873±0.004 | test=0.827±0.010\n",
      "\n",
      "-- RAPPORT (seuil 0.6) --\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.950     0.611     0.744       247\n",
      "           1      0.289     0.830     0.429        47\n",
      "\n",
      "    accuracy                          0.646       294\n",
      "   macro avg      0.619     0.721     0.586       294\n",
      "weighted avg      0.844     0.646     0.693       294\n",
      "\n",
      "-- MATRICE DE CONFUSION --\n",
      "[[151  96]\n",
      " [  8  39]]\n",
      "-- AUCs (seuil-indep.) --\n",
      "ROC AUC (test) = 0.820\n",
      "PR AUC  (test) = 0.589\n"
     ]
    }
   ],
   "source": [
    "#RegressionLogistique\n",
    "\n",
    "# --- 2) Préparation des données d’entrée \n",
    "data_pd = data.copy()\n",
    "\n",
    "X = data_pd.drop(columns=[\"a_quitte_l_entreprise\"])\n",
    "y = data_pd[\"a_quitte_l_entreprise\"].map({\"Non\": 0, \"Oui\": 1}).astype(int)\n",
    "\n",
    "# --- 3) Pipeline complète avec TON FE + préprocessing classique + XGB ---\n",
    "numeric_sel = selector(dtype_include=[\"number\", \"bool\"])   # inclut les bools comme numériques\n",
    "categorical_sel = selector(dtype_exclude=[\"number\", \"bool\"])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler())   # centrage + mise à l'échelle des numériques\n",
    "        ]), numeric_sel),\n",
    "        (\"cat\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        ]), categorical_sel),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "pipe_logit = Pipeline(steps=[\n",
    "    (\"fe\", FeatureEngineer()),   # <- feature engineering, sans fuite\n",
    "    (\"prep\", preprocess),\n",
    "    (\"clf\", LogisticRegression(class_weight=\"balanced\", max_iter=2000)),\n",
    "])\n",
    "\n",
    "# --- 4A) En TRAIN / TEST \"classique\" (pas de fuite) ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "pipe_logit.fit(X_train, y_train)\n",
    "yp_logit_proba = pipe_logit.predict_proba(X_test)[:, 1]\n",
    "thr = 0.35  # ou ce que tu as choisi\n",
    "yp_logit = (yp_logit_proba >= thr).astype(int)\n",
    "\n",
    "\n",
    "print(\"\\n=== LogisticRegression (balanced) ===\")\n",
    "print(\"Accuracy           :\", accuracy_score(y_test, yp_logit))\n",
    "print(\"Balanced accuracy  :\", balanced_accuracy_score(y_test, yp_logit))\n",
    "print(\"Avg Precision (PR) :\", average_precision_score(y_test, yp_logit_proba))\n",
    "print(\"ROC AUC            :\", roc_auc_score(y_test, yp_logit_proba))\n",
    "print(\"Report:\\n\", classification_report(y_test, yp_logit))\n",
    "print(\"Confusion:\\n\", confusion_matrix(y_test, yp_logit))\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scoring = (\"precision\", \"recall\", \"roc_auc\")\n",
    "scores = cross_validate(\n",
    "    pipe_logit, X, y, cv=cv,\n",
    "    scoring=(\"precision\", \"recall\", \"roc_auc\"),\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "for m in scoring:\n",
    "    tr_mean, tr_std = scores[f\"train_{m}\"].mean(), scores[f\"train_{m}\"].std()\n",
    "    te_mean, te_std = scores[f\"test_{m}\"].mean(), scores[f\"test_{m}\"].std()\n",
    "    print(f\"[{m}] train={tr_mean:.3f}±{tr_std:.3f} | test={te_mean:.3f}±{te_std:.3f}\")\n",
    "\n",
    "proba_te = pipe_logit.predict_proba(X_test)[:, 1]\n",
    "y_pred_06 = (proba_te >= thr).astype(int)\n",
    "\n",
    "print(\"\\n-- RAPPORT (seuil 0.6) --\")\n",
    "print(classification_report(y_test, y_pred_06, digits=3, zero_division=0))\n",
    "\n",
    "print(\"-- MATRICE DE CONFUSION --\")\n",
    "print(confusion_matrix(y_test, y_pred_06))  # [[tn, fp], [fn, tp]]\n",
    "\n",
    "print(\"-- AUCs (seuil-indep.) --\")\n",
    "print(f\"ROC AUC (test) = {roc_auc_score(y_test, proba_te):.3f}\")\n",
    "print(f\"PR AUC  (test) = {average_precision_score(y_test, proba_te):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f5488b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ForetAleatoire ===\n",
      "Accuracy           : 0.826530612244898\n",
      "Balanced accuracy  : 0.5522008786286502\n",
      "Avg Precision (PR) : 0.40853023868132476\n",
      "ROC AUC            : 0.7931777069515031\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.96      0.90       247\n",
      "           1       0.39      0.15      0.22        47\n",
      "\n",
      "    accuracy                           0.83       294\n",
      "   macro avg       0.62      0.55      0.56       294\n",
      "weighted avg       0.78      0.83      0.79       294\n",
      "\n",
      "Confusion:\n",
      " [[236  11]\n",
      " [ 40   7]]\n"
     ]
    }
   ],
   "source": [
    "#ForetAleatoire\n",
    "\n",
    "# --- 2) Préparation des données d’entrée \n",
    "data_pd = data.copy()\n",
    "\n",
    "X = data_pd.drop(columns=[\"a_quitte_l_entreprise\"])\n",
    "y = data_pd[\"a_quitte_l_entreprise\"].map({\"Non\": 0, \"Oui\": 1}).astype(int)\n",
    "\n",
    "# --- 3) Pipeline complète avec TON FE + préprocessing classique + XGB ---\n",
    "numeric_sel = selector(dtype_include=[\"number\", \"bool\"])   # inclut les bools comme numériques\n",
    "categorical_sel = selector(dtype_exclude=[\"number\", \"bool\"])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler())   # centrage + mise à l'échelle des numériques\n",
    "        ]), numeric_sel),\n",
    "        (\"cat\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        ]), categorical_sel),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "pipe_random = Pipeline(steps=[\n",
    "    (\"fe\", FeatureEngineer()),   # <- feature engineering, sans fuite\n",
    "    (\"prep\", preprocess),\n",
    "    (\"rf\", RandomForestClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=2,\n",
    "    class_weight=\"balanced_subsample\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")),\n",
    "])\n",
    "\n",
    "# --- 4A) En TRAIN / TEST \"classique\" (pas de fuite) ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "pipe_random.fit(X_train, y_train)\n",
    "yp_random = pipe_random.predict(X_test)\n",
    "yp_random_proba = pipe_random.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\n=== ForetAleatoire ===\")\n",
    "print(\"Accuracy           :\", accuracy_score(y_test, yp_random))\n",
    "print(\"Balanced accuracy  :\", balanced_accuracy_score(y_test, yp_random))\n",
    "print(\"Avg Precision (PR) :\", average_precision_score(y_test, yp_random_proba))\n",
    "print(\"ROC AUC            :\", roc_auc_score(y_test, yp_random_proba))\n",
    "print(\"Report:\\n\", classification_report(y_test, yp_random))\n",
    "print(\"Confusion:\\n\", confusion_matrix(y_test, yp_random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f1e3a3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HistGradientBoosting ===\n",
      "Accuracy           : 0.8503401360544217\n",
      "Balanced accuracy  : 0.6180549573606684\n",
      "Avg Precision (PR) : 0.506492658480869\n",
      "ROC AUC            : 0.8271168920665002\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.92       247\n",
      "           1       0.57      0.28      0.37        47\n",
      "\n",
      "    accuracy                           0.85       294\n",
      "   macro avg       0.72      0.62      0.64       294\n",
      "weighted avg       0.83      0.85      0.83       294\n",
      "\n",
      "Confusion:\n",
      " [[237  10]\n",
      " [ 34  13]]\n"
     ]
    }
   ],
   "source": [
    "#HistGradientBoosting\n",
    "\n",
    "# --- 2) Préparation des données d’entrée \n",
    "data_pd = data.copy()\n",
    "\n",
    "X = data_pd.drop(columns=[\"a_quitte_l_entreprise\"])\n",
    "y = data_pd[\"a_quitte_l_entreprise\"].map({\"Non\": 0, \"Oui\": 1}).astype(int)\n",
    "\n",
    "# --- 3) Pipeline complète avec TON FE + préprocessing classique + XGB ---\n",
    "numeric_sel = selector(dtype_include=[\"number\", \"bool\"])   # inclut les bools comme numériques\n",
    "categorical_sel = selector(dtype_exclude=[\"number\", \"bool\"])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler())   # centrage + mise à l'échelle des numériques\n",
    "        ]), numeric_sel),\n",
    "        (\"cat\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        ]), categorical_sel),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "pipe_hgb = Pipeline(steps=[\n",
    "    (\"fe\", FeatureEngineer()),   # <- feature engineering, sans fuite\n",
    "    (\"prep\", preprocess),\n",
    "    (\"hgb\", HistGradientBoostingClassifier(\n",
    "    max_depth=None,\n",
    "    learning_rate=0.1,\n",
    "    max_iter=300,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1\n",
    "    # (pas de class_weight ici; on compensera avec la métrique choisie et, plus tard, le tuning)\n",
    ")),\n",
    "])\n",
    "\n",
    "# --- 4A) En TRAIN / TEST \"classique\" (pas de fuite) ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "pipe_hgb.fit(X_train, y_train)\n",
    "yp_hgb = pipe_hgb.predict(X_test)\n",
    "yp_hgb_proba = pipe_hgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\n=== HistGradientBoosting ===\")\n",
    "print(\"Accuracy           :\", accuracy_score(y_test, yp_hgb))\n",
    "print(\"Balanced accuracy  :\", balanced_accuracy_score(y_test, yp_hgb))\n",
    "print(\"Avg Precision (PR) :\", average_precision_score(y_test, yp_hgb_proba))\n",
    "print(\"ROC AUC            :\", roc_auc_score(y_test, yp_hgb_proba))\n",
    "print(\"Report:\\n\", classification_report(y_test, yp_hgb))\n",
    "print(\"Confusion:\\n\", confusion_matrix(y_test, yp_hgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "71b867af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[precision] train=1.000±0.000 | test=0.634±0.062\n",
      "[recall] train=1.000±0.000 | test=0.350±0.067\n",
      "[roc_auc] train=1.000±0.000 | test=0.820±0.025\n",
      "\n",
      "-- RAPPORT (seuil 0.6) --\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.873     0.976     0.922       247\n",
      "           1      0.667     0.255     0.369        47\n",
      "\n",
      "    accuracy                          0.861       294\n",
      "   macro avg      0.770     0.616     0.645       294\n",
      "weighted avg      0.840     0.861     0.833       294\n",
      "\n",
      "-- MATRICE DE CONFUSION --\n",
      "[[241   6]\n",
      " [ 35  12]]\n",
      "-- AUCs (seuil-indep.) --\n",
      "ROC AUC (test) = 0.817\n",
      "PR AUC  (test) = 0.493\n"
     ]
    }
   ],
   "source": [
    "# --- 2) Préparation des données d’entrée (pandas) ---\n",
    "data_pd = data.copy()\n",
    "\n",
    "X = data_pd.drop(columns=[\"a_quitte_l_entreprise\"])\n",
    "y = data_pd[\"a_quitte_l_entreprise\"].map({\"Non\": 0, \"Oui\": 1}).astype(int)\n",
    "\n",
    "# --- 3) Pipeline complète avec TON FE + préprocessing classique + XGB ---\n",
    "numeric_sel = selector(dtype_include=[\"number\", \"bool\"])   # inclut les bools comme numériques\n",
    "categorical_sel = selector(dtype_exclude=[\"number\", \"bool\"])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler())   # centrage + mise à l'échelle des numériques\n",
    "        ]), numeric_sel),\n",
    "        (\"cat\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        ]), categorical_sel),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "    (\"fe\", FeatureEngineer()),   # <- ton feature engineering, sans fuite\n",
    "    (\"prep\", preprocess),\n",
    "    (\"xgb\", XGBClassifier(\n",
    "        random_state=42,\n",
    "        eval_metric=\"logloss\",\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        n_jobs=-1\n",
    "    )),\n",
    "])\n",
    "\n",
    "# --- 4A) En TRAIN / TEST \"classique\" (pas de fuite) ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "pipe.fit(X_train, y_train)              \n",
    "\n",
    "# --- 4B) En Cross-Validation stratifiée (pas de fuite) ---\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scoring = (\"precision\", \"recall\", \"roc_auc\")\n",
    "scores = cross_validate(\n",
    "    pipe, X, y, cv=cv,\n",
    "    scoring=(\"precision\", \"recall\", \"roc_auc\"),\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "for m in scoring:\n",
    "    tr_mean, tr_std = scores[f\"train_{m}\"].mean(), scores[f\"train_{m}\"].std()\n",
    "    te_mean, te_std = scores[f\"test_{m}\"].mean(), scores[f\"test_{m}\"].std()\n",
    "    print(f\"[{m}] train={tr_mean:.3f}±{tr_std:.3f} | test={te_mean:.3f}±{te_std:.3f}\")\n",
    "\n",
    "proba_te = pipe.predict_proba(X_test)[:, 1]\n",
    "y_pred_06 = (proba_te >= 0.6).astype(int)\n",
    "\n",
    "print(\"\\n-- RAPPORT (seuil 0.6) --\")\n",
    "print(classification_report(y_test, y_pred_06, digits=3, zero_division=0))\n",
    "\n",
    "print(\"-- MATRICE DE CONFUSION --\")\n",
    "print(confusion_matrix(y_test, y_pred_06))  # [[tn, fp], [fn, tp]]\n",
    "\n",
    "print(\"-- AUCs (seuil-indep.) --\")\n",
    "print(f\"ROC AUC (test) = {roc_auc_score(y_test, proba_te):.3f}\")\n",
    "print(f\"PR AUC  (test) = {average_precision_score(y_test, proba_te):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c2c54023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Permutation importance (PR AUC) — top 30 :\n",
      "                                  feature  importance_mean  importance_std\n",
      "                    heure_supplementaires         0.228049        0.030934\n",
      "              annees_dans_le_poste_actuel         0.090910        0.038253\n",
      "           nombre_experiences_precedentes         0.090527        0.022038\n",
      "                           revenu_mensuel         0.081878        0.039725\n",
      "      satisfaction_employee_environnement         0.062482        0.026376\n",
      "      annees_depuis_la_derniere_promotion         0.061184        0.022201\n",
      "     satisfaction_employee_nature_travail         0.058355        0.019396\n",
      "                           statut_marital         0.052569        0.016940\n",
      "             satisfaction_employee_equipe         0.035524        0.018496\n",
      "               note_evaluation_precedente         0.028812        0.021726\n",
      "                  annee_experience_totale         0.022268        0.024387\n",
      "                distance_domicile_travail         0.021239        0.016824\n",
      "                                    genre         0.019837        0.009816\n",
      "         augementation_salaire_precedente         0.019793        0.029326\n",
      "                              departement         0.015417        0.021636\n",
      "satisfaction_employee_equilibre_pro_perso         0.012309        0.011476\n",
      "                    nb_formations_suivies         0.003931        0.008548\n",
      "                 note_evaluation_actuelle         0.002409        0.005316\n",
      "                niveau_hierarchique_poste         0.001403        0.020537\n",
      "                    frequence_deplacement         0.000000        0.000000\n",
      "                                    poste         0.000000        0.000000\n",
      "                            domaine_etude         0.000000        0.000000\n",
      "                 nombre_participation_pee        -0.000902        0.004739\n",
      "                         niveau_education        -0.000904        0.002452\n",
      "            annes_sous_responsable_actuel        -0.003053        0.017337\n",
      "                                      age        -0.007728        0.010546\n",
      "                 annees_dans_l_entreprise        -0.009608        0.026379\n"
     ]
    }
   ],
   "source": [
    "# importance selon la PR AUC (plus sensible aux positifs rares)\n",
    "perm = permutation_importance(\n",
    "    pipe_logit, X_test, y_test,\n",
    "    scoring=\"average_precision\",\n",
    "    n_repeats=20, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "perm_imp = (\n",
    "    pd.DataFrame({\n",
    "        \"feature\": X_test.columns,\n",
    "        \"importance_mean\": perm.importances_mean,\n",
    "        \"importance_std\": perm.importances_std\n",
    "    })\n",
    "    .sort_values(\"importance_mean\", ascending=False)\n",
    ")\n",
    "\n",
    "print(\"\\nPermutation importance (PR AUC) — top 30 :\")\n",
    "print(perm_imp.head(50).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "272471f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Permutation importance (PR AUC) — top 50 features transformées :\n",
      "                                   feature  importance_mean  importance_std\n",
      "                      num__heure_supp_flag         0.228049        0.030934\n",
      "       num__nombre_experiences_precedentes         0.090527        0.022038\n",
      "                       num__revenu_mensuel         0.079729        0.031512\n",
      "            num__niveau_hierarchique_poste         0.070263        0.023377\n",
      "  num__annees_depuis_la_derniere_promotion         0.054072        0.021973\n",
      "                             num__sat_mean         0.042738        0.021162\n",
      "                num__a_connu_mvmnt_interne         0.033117        0.015949\n",
      " num__satisfaction_employee_nature_travail         0.028808        0.011733\n",
      "            num__distance_domicile_travail         0.028185        0.018641\n",
      "                num__nb_formations_suivies         0.025324        0.013012\n",
      "  num__satisfaction_employee_environnement         0.022140        0.015196\n",
      "                   num__revenu_vs_dept_med         0.020809        0.015960\n",
      "           cat__statut_marital_Célibataire         0.017277        0.013531\n",
      "            num__tenure_ratio_current_post         0.015395        0.023277\n",
      "                         num__sat_low_flag         0.014797        0.009569\n",
      "            cat__statut_marital_Divorcé(e)         0.013802        0.007508\n",
      "cat__augementation_salaire_precedente_11 %         0.012556        0.014924\n",
      "                              num__sat_min         0.011507        0.006928\n",
      "                              num__sat_std         0.008890        0.011687\n",
      "           num__note_evaluation_precedente         0.008780        0.012615\n",
      "            num__nb_annnee_hors_entreprise         0.008620        0.006319\n",
      "          num__annees_dans_le_poste_actuel         0.007512        0.010102\n",
      "                              cat__genre_F         0.007418        0.005273\n",
      "cat__augementation_salaire_precedente_19 %         0.005947        0.008388\n",
      "             num__note_evaluation_actuelle         0.005126        0.008354\n",
      "              num__annee_experience_totale         0.005089        0.008843\n",
      "                              cat__genre_M         0.004767        0.005057\n",
      "cat__augementation_salaire_precedente_15 %         0.004222        0.004284\n",
      "         num__satisfaction_employee_equipe         0.004123        0.005538\n",
      "                    num__recent_promo_flag         0.003516        0.002322\n",
      "cat__augementation_salaire_precedente_17 %         0.003329        0.001499\n",
      "cat__augementation_salaire_precedente_13 %         0.002825        0.003410\n",
      "cat__augementation_salaire_precedente_25 %         0.001192        0.003606\n",
      "cat__augementation_salaire_precedente_12 %         0.001044        0.000796\n",
      "cat__augementation_salaire_precedente_24 %         0.001012        0.000482\n",
      "cat__augementation_salaire_precedente_23 %         0.000564        0.001127\n",
      "cat__augementation_salaire_precedente_18 %         0.000128        0.001141\n",
      "                     num__perf_degrade_niv         0.000065        0.001162\n",
      "                    num__long_commute_flag         0.000007        0.003537\n",
      "      cat__departement_Ressources Humaines        -0.000046        0.000606\n",
      "             num__annees_dans_l_entreprise        -0.000209        0.002536\n",
      "                num__delta_note_evaluation        -0.000410        0.007373\n",
      "cat__augementation_salaire_precedente_22 %        -0.000625        0.012357\n",
      "             num__nombre_participation_pee        -0.000902        0.004739\n",
      "                     num__niveau_education        -0.000904        0.002452\n",
      "             num__Ecart_nb_annee_sur_poste        -0.001176        0.006942\n",
      "cat__augementation_salaire_precedente_20 %        -0.001643        0.001774\n",
      "              cat__statut_marital_Marié(e)        -0.001772        0.004588\n",
      "cat__augementation_salaire_precedente_21 %        -0.001873        0.002567\n",
      "        num__annes_sous_responsable_actuel        -0.003053        0.017337\n",
      "\n",
      "Permutation importance agrégée par variable source (somme des OHE) — top 50 :\n",
      "                                   source  importance_mean\n",
      "      satisfaction_employee_environnement         0.228049\n",
      "               note_evaluation_precedente         0.090527\n",
      "                niveau_hierarchique_poste         0.079729\n",
      "     satisfaction_employee_nature_travail         0.070263\n",
      "             satisfaction_employee_equipe         0.054072\n",
      "satisfaction_employee_equilibre_pro_perso         0.042738\n",
      "                 note_evaluation_actuelle         0.033117\n",
      "                                      age         0.028808\n",
      "                           revenu_mensuel         0.028185\n",
      "           nombre_experiences_precedentes         0.025324\n",
      "                  annee_experience_totale         0.022140\n",
      "                 annees_dans_l_entreprise         0.020809\n",
      "              annees_dans_le_poste_actuel         0.017277\n",
      "                 nombre_participation_pee         0.015395\n",
      "                    nb_formations_suivies         0.014797\n",
      "                distance_domicile_travail         0.013802\n",
      "                         niveau_education         0.012556\n",
      "      annees_depuis_la_derniere_promotion         0.011507\n",
      "            annes_sous_responsable_actuel         0.008890\n",
      "                    delta_note_evaluation         0.008780\n",
      "                         perf_degrade_niv         0.008620\n",
      "                nb_annnee_hors_entreprise         0.007512\n",
      "            ratio_dans_et_hors_entreprise         0.007418\n",
      "              hors_entreprise_majoritaire         0.005947\n",
      "                 Ecart_nb_annee_sur_poste         0.005126\n",
      "                    a_connu_mvmnt_interne         0.005089\n",
      "                tenure_ratio_current_post         0.004767\n",
      "                                 sat_mean         0.004222\n",
      "                                  sat_min         0.004123\n",
      "                                  sat_std         0.003516\n",
      "                             sat_low_flag         0.003329\n",
      "                       revenu_vs_dept_med         0.002825\n",
      "                     revenu_vs_niveau_med         0.001192\n",
      "                          heure_supp_flag         0.001044\n",
      "                        long_commute_flag         0.001012\n",
      "                        formations_par_an         0.000564\n",
      "                        recent_promo_flag         0.000128\n",
      "                                    genre        -0.012703\n",
      "                            augementation        -0.021906\n",
      "                                   statut        -0.024794\n",
      "                              departement        -0.057643\n"
     ]
    }
   ],
   "source": [
    "# 1) Sépare le pipeline : (fe + prep) puis le modèle seul\n",
    "fe_prep = pipe_logit[:-1]                 # garde \"fe\" + \"prep\"\n",
    "clf = pipe_logit.named_steps[\"clf\"]       # modèle entraîné (LogisticRegression ici)\n",
    "\n",
    "# 2) Transforme X_test au niveau features (après FE+preprocess)\n",
    "Xt_test = fe_prep.transform(X_test)\n",
    "\n",
    "# Si sortie sparse (probable à cause de l'OHE), on densifie juste pour la permutation\n",
    "# (le jeu de test est petit => pas de souci mémoire)\n",
    "if hasattr(Xt_test, \"toarray\"):\n",
    "    Xt_test_dense = Xt_test.toarray()\n",
    "else:\n",
    "    Xt_test_dense = Xt_test\n",
    "\n",
    "# 3) Récupère les noms de *toutes* les features après preprocess (num + OHE)\n",
    "feat_names = pipe_logit.named_steps[\"prep\"].get_feature_names_out()\n",
    "\n",
    "# 4) Permutation importance AU NIVEAU TRANSFORMÉ (on permute les colonnes de Xt_test_dense)\n",
    "perm = permutation_importance(\n",
    "    clf, Xt_test_dense, y_test,\n",
    "    scoring=\"average_precision\",   # PR AUC (sensible aux positifs rares)\n",
    "    n_repeats=20, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "imp_df = (\n",
    "    pd.DataFrame({\n",
    "        \"feature\": feat_names,\n",
    "        \"importance_mean\": perm.importances_mean,\n",
    "        \"importance_std\": perm.importances_std\n",
    "    })\n",
    "    .sort_values(\"importance_mean\", ascending=False)\n",
    ")\n",
    "\n",
    "print(\"\\nPermutation importance (PR AUC) — top 50 features transformées :\")\n",
    "print(imp_df.head(50).to_string(index=False))\n",
    "\n",
    "# 5) (Optionnel) Agréger par variable *source* (avant OHE)\n",
    "def source_from_feature(transformed_name: str) -> str:\n",
    "    # ColumnTransformer met des préfixes \"num__\" / \"cat__\"\n",
    "    base = transformed_name.split(\"__\", 1)[-1]\n",
    "    # Les colonnes OHE ressemblent à \"cat__poste_Agent\" -> on coupe au premier \"_\"\n",
    "    # ⚠️ Si tes noms sources contiennent déjà des \"_\" (ex: \"note_actuelle\"),\n",
    "    # adapte cette logique (par ex. split sur un séparateur personnalisé).\n",
    "    if transformed_name.startswith(\"cat__\"):\n",
    "        return base.split(\"_\", 1)[0]\n",
    "    return base\n",
    "\n",
    "imp_df[\"source\"] = [source_from_feature(n) for n in feat_names]\n",
    "\n",
    "agg_df = (\n",
    "    imp_df.groupby(\"source\", as_index=False)[\"importance_mean\"]\n",
    "          .sum()\n",
    "          .sort_values(\"importance_mean\", ascending=False)\n",
    ")\n",
    "\n",
    "print(\"\\nPermutation importance agrégée par variable source (somme des OHE) — top 50 :\")\n",
    "print(agg_df.head(500).to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Projet 4 (Poetry)",
   "language": "python",
   "name": "projet4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
